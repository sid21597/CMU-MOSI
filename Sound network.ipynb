{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "changing-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from matplotlib.image import imread\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization,Activation,Dropout\n",
    "from tensorflow.keras.layers import Dense,LeakyReLU\n",
    "import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,MaxPool2D,AveragePooling2D,GlobalAveragePooling2D,MaxPooling1D\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense,BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn import metrics\n",
    "from keras import Sequential\n",
    "from matplotlib import pyplot as pt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.initializers import *\n",
    "from keras.losses import binary_crossentropy\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "import sklearn.metrics as metrics\n",
    "import random\n",
    "import cv2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statistical-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = 'D:\\CMUSUER\\Sound\\Train'\n",
    "trainarr = os.listdir(trainpath)\n",
    "validpath = 'D:\\CMUSUER\\Sound\\Valid'\n",
    "validarr = os.listdir(validpath)\n",
    "testpath = 'D:\\CMUSUER\\Sound\\Test'\n",
    "testarr = os.listdir(testpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "negative-sponsorship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '0909', '28', '1.20']\n",
      "1iG0909rllw_28&1.20.jpg\n",
      "1\n",
      "1.2\n"
     ]
    }
   ],
   "source": [
    "i = 60\n",
    "b = re.findall('\\d*\\.?\\d+',trainarr[i])\n",
    "print(b)\n",
    "print(trainarr[i])\n",
    "c = trainarr[i].split(\"&\", 1)[-1][0]\n",
    "print(c)\n",
    "if c == \"-\":\n",
    "    b = float(b[-1])*-1\n",
    "else:\n",
    "    b = float(b[-1])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worth-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "train_labels = []\n",
    "for i in range(len (trainarr)):\n",
    "  myFile = os.path.join(trainpath, trainarr[i])\n",
    "  image = cv2.imread(myFile)\n",
    "  image1 = cv2.resize(image,(128,128), interpolation=cv2.INTER_CUBIC)\n",
    "  train.append(image1)\n",
    "  b = re.findall('\\d*\\.?\\d+',trainarr[i])\n",
    "  c = trainarr[i].split(\"&\", 1)[-1][0]\n",
    "  if c == \"-\":\n",
    "      b = float(b[-1])*-1\n",
    "  else:\n",
    "      b = float(b[-1])\n",
    "  train_labels.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "complimentary-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = []\n",
    "valid_labels = []\n",
    "for i in range(len (validarr)):\n",
    "  myFile = os.path.join(validpath, validarr[i])\n",
    "  image = cv2.imread(myFile)\n",
    "  image1 = cv2.resize(image,(128,128), interpolation=cv2.INTER_CUBIC)\n",
    "  valid.append(image1)\n",
    "  b = re.findall('\\d*\\.?\\d+',validarr[i])\n",
    "  c = validarr[i].split(\"&\", 1)[-1][0]\n",
    "  if c == \"-\":\n",
    "      b = float(b[-2])*-1\n",
    "  else:\n",
    "      b = float(b[-2])\n",
    "  valid_labels.append(b)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "southwest-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "test_labels = []\n",
    "for i in range(len (testarr)):\n",
    "  myFile = os.path.join(testpath, testarr[i])\n",
    "  image = cv2.imread(myFile)\n",
    "  image1 = cv2.resize(image,(128,128), interpolation=cv2.INTER_CUBIC)\n",
    "  test.append(image1)\n",
    "  b = re.findall('\\d*\\.?\\d+',testarr[i])\n",
    "  c = testarr[i].split(\"&\", 1)[-1][0]\n",
    "  if c == \"-\":\n",
    "      b = float(b[-2])*-1\n",
    "  else:\n",
    "      b = float(b[-2])\n",
    "  test_labels.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "trained-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.asarray(train)\n",
    "train_labels = np.asarray(train_labels)\n",
    "valid = np.asarray(valid)\n",
    "valid_labels = np.asarray(valid_labels)\n",
    "test = np.asarray(test)\n",
    "test_labels = np.asarray(test_labels)\n",
    "train = train/255\n",
    "valid = valid/255\n",
    "test = test/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "activated-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels1 = []\n",
    "for i in train_labels:\n",
    "    temp = ((2*(i-min(train_labels)))/(max(train_labels)-min(train_labels)))-1\n",
    "    train_labels1.append(temp)\n",
    "valid_labels1 = []\n",
    "for i in valid_labels:\n",
    "    temp = ((2*(i-min(valid_labels)))/(max(valid_labels)-min(valid_labels)))-1\n",
    "    valid_labels1.append(temp)\n",
    "test_labels1 = []\n",
    "for i in test_labels:\n",
    "    temp = ((2*(i-min(test_labels)))/(max(test_labels)-min(test_labels)))-1\n",
    "    test_labels1.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbd954e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels1 = np.asarray(train_labels1)\n",
    "valid_labels1 = np.asarray(valid_labels1)\n",
    "test_labels1 = np.asarray(test_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "worth-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "  classifier = tensorflow.keras.Sequential()\n",
    "  classifier.add(Convolution2D(input_shape=(128,128,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"tanh\"))\n",
    "  classifier.add(Conv2D(filters=16,kernel_size=(3,3),padding=\"same\", activation=\"tanh\"))\n",
    "  classifier.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "  classifier.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"tanh\"))\n",
    "  classifier.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"tanh\"))\n",
    "  classifier.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "  classifier.add(Conv2D(filters=16, kernel_size=(3,3), padding=\"same\", activation=\"tanh\"))\n",
    "\n",
    "  classifier.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "  classifier.add(Flatten())\n",
    "\n",
    "  classifier.add(Dense(units=16,activation = \"tanh\"))\n",
    "  classifier.add(Dense(units=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "personalized-yemen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001C0EACF9EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001C0EACF9EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      " 2/21 [=>............................] - ETA: 2s - loss: 0.2585 - mse: 0.2585 - mae: 0.4497WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0598s vs `on_train_batch_end` time: 0.1188s). Check your callbacks.\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2552 - mse: 0.2552 - mae: 0.4397- ETA: 0s - loss: 0.2571 - mse: 0.2571 - mae: 0.44WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001C0D2B20AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001C0D2B20AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "21/21 [==============================] - 4s 182ms/step - loss: 0.2555 - mse: 0.2555 - mae: 0.4402 - val_loss: 0.2514 - val_mse: 0.2514 - val_mae: 0.4187\n",
      "Epoch 2/5\n",
      "21/21 [==============================] - 4s 177ms/step - loss: 0.2564 - mse: 0.2564 - mae: 0.4417 - val_loss: 0.2483 - val_mse: 0.2483 - val_mae: 0.4178\n",
      "Epoch 3/5\n",
      "21/21 [==============================] - 4s 178ms/step - loss: 0.2550 - mse: 0.2550 - mae: 0.4409 - val_loss: 0.2483 - val_mse: 0.2483 - val_mae: 0.4182\n",
      "Epoch 4/5\n",
      "21/21 [==============================] - 4s 177ms/step - loss: 0.2546 - mse: 0.2546 - mae: 0.4409 - val_loss: 0.2501 - val_mse: 0.2501 - val_mae: 0.4181\n",
      "Epoch 5/5\n",
      "21/21 [==============================] - 4s 177ms/step - loss: 0.2537 - mse: 0.2537 - mae: 0.4396 - val_loss: 0.2488 - val_mse: 0.2488 - val_mae: 0.4177\n"
     ]
    }
   ],
   "source": [
    "opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)\n",
    "classifier.compile(optimizer = opt, loss = 'mse', metrics = ['mse','mae'])\n",
    "history = classifier.fit(train, train_labels1, batch_size=64, epochs=5, verbose=1,validation_data = (valid,valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "527922c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1535357282173552"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_labels1,classifier.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168e8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
