{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb763e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\CMUSUER\\Text\\Training.csv\")\n",
    "train = []\n",
    "train_labels = []\n",
    "for i in range(len(df)):\n",
    "    train.append(df.iloc[i][2])\n",
    "    train_labels.append(df.iloc[i][3])\n",
    "df = pd.read_csv(\"D:\\CMUSUER\\Text\\Valid.csv\")   \n",
    "valid = []\n",
    "valid_labels = []\n",
    "for i in range(len(df)):\n",
    "    valid.append(df.iloc[i][3])\n",
    "    valid_labels.append(df.iloc[i][4])\n",
    "df = pd.read_csv(\"D:\\CMUSUER\\Text\\Test.csv\")   \n",
    "test = []\n",
    "test_labels = []\n",
    "for i in range(len(df)):\n",
    "    test.append(df.iloc[i][3])\n",
    "    test_labels.append(df.iloc[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000\n",
    "embedding_dim = 32\n",
    "max_length = 100\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train = np.asarray(train)\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train)\n",
    "word_index = tokenizer.word_index\n",
    "train_sequences = tokenizer.texts_to_sequences(train)\n",
    "train_padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.asarray(test)\n",
    "valid  = np.asarray(valid)\n",
    "testing_sequences = tokenizer.texts_to_sequences(test)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid)\n",
    "valid_padded = pad_sequences(valid_sequences,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "valid_labels = np.asarray(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f4f15b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1 for GRU, Enter 2 for Bi-directional LSTM, Enter 3 for Convolutional layer 1\n",
      "Enter 'mse' for mean squared error loss or enter 'mae' for mean absolute error loss mae\n"
     ]
    }
   ],
   "source": [
    "model_sel = input(\"Enter 1 for GRU, Enter 2 for Bi-directional LSTM, Enter 3 for Convolutional layer \")\n",
    "loss_sel = input(\"Enter 'mse' for mean squared error loss or enter 'mae' for mean absolute error loss \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "381beb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001613EE23AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001613EE23AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "40/41 [============================>.] - ETA: 0s - loss: 1.3180 - mse: 2.2854 - mae: 1.3180WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001613EE03798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001613EE03798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 1.3188 - mse: 2.2870 - mae: 1.3188 - val_loss: 1.4061 - val_mse: 2.6928 - val_mae: 1.4061\n",
      "Epoch 2/20\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 1.2162 - mse: 2.0364 - mae: 1.2162 - val_loss: 1.2126 - val_mse: 2.2524 - val_mae: 1.2126\n",
      "Epoch 3/20\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.9344 - mse: 1.3867 - mae: 0.9344 - val_loss: 1.2097 - val_mse: 2.1119 - val_mae: 1.2097\n",
      "Epoch 4/20\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.7218 - mse: 0.9085 - mae: 0.7218 - val_loss: 1.1764 - val_mse: 2.0436 - val_mae: 1.1764\n",
      "Epoch 5/20\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.6104 - mse: 0.6996 - mae: 0.6104 - val_loss: 1.1434 - val_mse: 2.0614 - val_mae: 1.1434\n",
      "Epoch 6/20\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.5364 - mse: 0.5700 - mae: 0.5364 - val_loss: 1.1745 - val_mse: 2.0879 - val_mae: 1.1745\n",
      "Epoch 7/20\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.4937 - mse: 0.5059 - mae: 0.4937 - val_loss: 1.1474 - val_mse: 2.1128 - val_mae: 1.1474\n",
      "Epoch 8/20\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.4443 - mse: 0.4194 - mae: 0.4443 - val_loss: 1.1567 - val_mse: 2.1418 - val_mae: 1.1567\n",
      "Epoch 9/20\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.4098 - mse: 0.3802 - mae: 0.4098 - val_loss: 1.1487 - val_mse: 2.0803 - val_mae: 1.1487\n",
      "Epoch 10/20\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.3753 - mse: 0.3354 - mae: 0.3753 - val_loss: 1.1723 - val_mse: 2.1145 - val_mae: 1.1723\n",
      "Epoch 11/20\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.3730 - mse: 0.3269 - mae: 0.3730 - val_loss: 1.1608 - val_mse: 2.1529 - val_mae: 1.1608\n",
      "Epoch 12/20\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.3480 - mse: 0.2902 - mae: 0.3480 - val_loss: 1.1614 - val_mse: 2.1334 - val_mae: 1.1614\n",
      "Epoch 13/20\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.3261 - mse: 0.2721 - mae: 0.3261 - val_loss: 1.1784 - val_mse: 2.1996 - val_mae: 1.1784\n",
      "Epoch 14/20\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.3144 - mse: 0.2591 - mae: 0.3144 - val_loss: 1.1621 - val_mse: 2.1780 - val_mae: 1.1621\n",
      "Epoch 15/20\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.3120 - mse: 0.2468 - mae: 0.3120 - val_loss: 1.1624 - val_mse: 2.1629 - val_mae: 1.1624\n",
      "Epoch 16/20\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.3118 - mse: 0.2444 - mae: 0.3118 - val_loss: 1.1639 - val_mse: 2.1730 - val_mae: 1.1639\n",
      "Epoch 17/20\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.2958 - mse: 0.2329 - mae: 0.2958 - val_loss: 1.1760 - val_mse: 2.1662 - val_mae: 1.1760\n",
      "Epoch 18/20\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.2869 - mse: 0.2214 - mae: 0.2869 - val_loss: 1.1589 - val_mse: 2.1437 - val_mae: 1.1589\n",
      "Epoch 19/20\n",
      "41/41 [==============================] - 1s 23ms/step - loss: 0.2732 - mse: 0.2086 - mae: 0.2732 - val_loss: 1.1519 - val_mse: 2.1565 - val_mae: 1.1519\n",
      "Epoch 20/20\n",
      "41/41 [==============================] - 1s 22ms/step - loss: 0.2676 - mse: 0.2038 - mae: 0.2676 - val_loss: 1.1566 - val_mse: 2.1460 - val_mae: 1.1566\n"
     ]
    }
   ],
   "source": [
    "if(model_sel == '1'):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='%s'%loss_sel,optimizer='adam',metrics=['mse','mae'])\n",
    "    num_epochs = 20\n",
    "    history = model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(valid_padded, valid_labels))\n",
    "\n",
    "if(model_sel == '2'):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='%s'%loss_sel,optimizer='adam',metrics=['mse','mae'])\n",
    "    model.summary()\n",
    "    num_epochs = 15\n",
    "    history = model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(valid_padded, valid_labels))\n",
    "\n",
    "if(model_sel == '3'):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.Conv1D(128, 5, activation='linear'),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='%s'%loss_sel,optimizer='adam',metrics=['mse','mae'])\n",
    "    model.summary()\n",
    "    num_epochs = 15\n",
    "    history = model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(valid_padded, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4ca4823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 1.1721688022807824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "if loss_sel == 'mae':\n",
    "    print(\"MAE\" ,mean_absolute_error(test_labels,model.predict(testing_padded)))\n",
    "if loss_sel == 'mse':\n",
    "    print(\"MSE\",mean_squared_error(test_labels,model.predict(testing_padded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a6647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaabba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a7e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ddf30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
